{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2_Chatbot.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc6PNOT0mqAs"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7vC4o-2jShmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/kochat"
      ],
      "metadata": {
        "id": "U7_c65eBSvlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "metadata": {
        "id": "bAg-haUfmwWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
      ],
      "metadata": {
        "id": "22gK1Ea3mxUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.bos_token_id)\n",
        "print(tokenizer.eos_token_id)\n",
        "print(tokenizer.pad_token_id)\n",
        "print('-' * 10)\n",
        "print(tokenizer.decode(1))\n",
        "print(tokenizer.decode(2))\n",
        "print(tokenizer.decode(3))\n",
        "print(tokenizer.decode(4))"
      ],
      "metadata": {
        "id": "9G6OFuj5n5dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tqdm\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "dckISrGSmyQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('finetune.csv')"
      ],
      "metadata": {
        "id": "7KMXXlMYm8nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "fgBLUI34cHqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "RQf3VZM8dsuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chat_data():\n",
        "  for question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):\n",
        "    bos_token = [tokenizer.bos_token_id]\n",
        "    eos_token = [tokenizer.eos_token_id]\n",
        "    sent = tokenizer.encode('<usr>' + question + '<sys>' + answer) \n",
        "    yield bos_token + sent + eos_token"
      ],
      "metadata": {
        "id": "yX7-wj-vnG-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)"
      ],
      "metadata": {
        "id": "k9yWOljzniTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "0xG4Q_Jpnkh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataset:\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "id": "sin4lpyGfxzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(batch[0])"
      ],
      "metadata": {
        "id": "tToemB1If1Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch[0])"
      ],
      "metadata": {
        "id": "MknLzMf5f3Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode('</s><usr> 12시 땡!<sys> 하루가 또 가네요.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'))"
      ],
      "metadata": {
        "id": "jbSvBKhJdX_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
      ],
      "metadata": {
        "id": "OfdTxuYFok9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = len(train_data) // batch_size + 1\n",
        "print(steps)"
      ],
      "metadata": {
        "id": "Tmkqate7o571"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for batch in tqdm.tqdm_notebook(dataset, total=steps):\n",
        "      with tf.GradientTape() as tape:\n",
        "          result = model(batch, labels=batch)\n",
        "          loss = result[0]\n",
        "          batch_loss = tf.reduce_mean(loss)\n",
        "          \n",
        "      grads = tape.gradient(batch_loss, model.trainable_variables)\n",
        "      adam.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      epoch_loss += batch_loss / steps\n",
        "\n",
        "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"
      ],
      "metadata": {
        "id": "Ut3XaV1qo8LO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YfTKkoohZuJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '오늘도 좋은 하루!'"
      ],
      "metadata": {
        "id": "QyxDdtOapWat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = '<usr>' + text + '<sys>'"
      ],
      "metadata": {
        "id": "VCrTFarcqUD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
        "input_ids = tf.convert_to_tensor([input_ids])"
      ],
      "metadata": {
        "id": "CMAEUZyiqXQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=20, early_stopping=True, eos_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "D5ptIm69qZ9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = tokenizer.decode(output[0].numpy().tolist())"
      ],
      "metadata": {
        "id": "irT0H8nBqc3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence.split('<sys> ')[1].replace('</s>', '')"
      ],
      "metadata": {
        "id": "ZDXWzgzWqh53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=20, do_sample=True, top_k=20, num_return_sequences=3)\n",
        "tokenizer.decode(output[0].numpy().tolist())"
      ],
      "metadata": {
        "id": "k2qenmvIqoaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_answer_by_chatbot(user_text):\n",
        "  sent = '<usr>' + user_text + '<sys>'\n",
        "  input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
        "  input_ids = tf.convert_to_tensor([input_ids])\n",
        "  output = model.generate(input_ids, max_length=20, do_sample=True, repetition_penalty=1.2, num_return_sequences=2)\n",
        "  sentence = tokenizer.decode(output[0].numpy().tolist())\n",
        "  sentence_1 = tokenizer.decode(output[1].numpy().tolist())\n",
        "\n",
        "  chatbot_response = sentence.split('<sys> ')[1].replace('</s>', '').replace('<pad>', '')\n",
        "  chatbot_response_1 = sentence_1.split('<sys> ')[1].replace('</s>', '').replace('<pad>', '')\n",
        "\n",
        "  return (chatbot_response, chatbot_response_1) "
      ],
      "metadata": {
        "id": "vggXTY7grYgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = ['나랑 영화 보자', '밥 같이 먹을래?', '좀 이따가 밥먹을래?', '너 언제와?', '내일 시간 괜찮을까요?', '커피 한 잔 할까?']\n",
        "\n",
        "for text in test_list:\n",
        "    out1, out2 = return_answer_by_chatbot(text)\n",
        "    print(\"Input: \", text)\n",
        "    print(\"Output 1: \", out1)\n",
        "    print(\"Output 2: \", out2)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "kdK6SxmMrym3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vDqkSfJuffR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}